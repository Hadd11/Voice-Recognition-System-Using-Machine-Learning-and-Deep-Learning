{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hadd11/Voice-Recognition-System-Using-Machine-Learning-and-Deep-Learning/blob/main/Grad_project_codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import csv\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "#Keras\n",
        "import keras\n",
        "import glob"
      ],
      "metadata": {
        "id": "7kYWoUodUjae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "metadata": {
        "id": "6vkt8Q49VzXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=pd.read_csv('/content/OurData.csv')\n",
        " array = dataset.values\n",
        " X = array[0:,1:]\n",
        " Y = array[0:,0]\n",
        " lb = LabelEncoder()\n",
        " Y = to_categorical(lb.fit_transform(Y))\n",
        " X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1,\n",
        "               random_state=150)\n",
        " X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        " test_size=0.1, random_state=12)\n",
        " ss = StandardScaler()\n",
        " X_train = ss.fit_transform(X_train)\n",
        " X_val = ss.transform(X_val)\n",
        " X_test = ss.transform(X_test)\n",
        " np.unique(y_test).shape"
      ],
      "metadata": {
        "id": "GqmxIWLwUjra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " directories = [\"Same\",\"Diff\", \"Arabic\"]\n",
        " for d in directories:\n",
        "   path = r'/home/walker/Desktop/2Channels/'+ d + \"/\"\n",
        "   for file_name in os.listdir(path):\n",
        "      path2 = path + file_name + \"/\"\n",
        "      for samples in os.listdir(path2):\n",
        "        audio = AudioSegment.from_file(path2+samples)\n",
        "        if audio.channels > 1 :\n",
        "            channels = audio.split_to_mono()\n",
        "            audio = channels[0]\n",
        "        PathToExport = \"~/Desktop/1Channel/\"+ d + '/' + file_name\n",
        "        if not os.path.isdir(PathToExport):\n",
        "             os.mkdir(PathToExport)\n",
        "        audio.export(PathToExport + \"/\" + samples , format=\"flac\")"
      ],
      "metadata": {
        "id": "2TMv7pOJN5jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Extract the statistical features\n",
        " for flac_path in flac_list:\n",
        "   speaker_id = flac_path.split('/')[4]\n",
        "   y, sr =librosa.load(flac_path)\n",
        "   chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "   rms = librosa.feature.rms(y=y)\n",
        "   spec_cent = librosa.feature.spectral_centroid(y=y,sr=sr)\n",
        "   spec_bw = librosa.feature.spectral_bandwidth(y=y,sr=sr)\n",
        "   rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "   spectral_contrast=librosa.feature.spectral_contrast(y)\n",
        "   spectral_flatness=librosa.feature.spectral_flatness(y)\n",
        "   zcr = librosa.feature.zero_crossing_rate(y)\n",
        "   mean = np.mean(y)\n",
        "   std = np.std(y)\n",
        "   maxv = np.amax(y)\n",
        "   minv = np.amin(y)\n",
        "   median = np.median(y)\n",
        "   skew = scipy.stats.skew(y)\n",
        "   kurt = scipy.stats.kurtosis(y)\n",
        "   iqr = scipy.stats.iqr(y)\n",
        "   mode = np.array(scipy.stats.mode(y))[0]"
      ],
      "metadata": {
        "id": "I5KMzA_uN6To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['linear']}"
      ],
      "metadata": {
        "id": "966GZY-zUQT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " corr = feature.corr()\n",
        " columns = np.full((corr.shape[0],), True, dtype=bool)\n",
        " for i in range(corr.shape[0]):\n",
        "   for j in range(i+1, corr.shape[0]):\n",
        "    if corr.iloc[i,j] >= 0.9:\n",
        "     if columns[j]:\n",
        "        columns[j] = False\n",
        " selected_columns = features.columns[columns]\n",
        " Features_After_Corr = features[selected_columns]\n",
        " data_after_Corr = pd.DataFrame()\n",
        " data_after_Corr = pd.DataFrame(data = Features_After_Corr.values ,\n",
        "    columns = selected_columns)\n",
        " data_after_Corr['id'] = IDs"
      ],
      "metadata": {
        "id": "pnr4JSIhRpM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymrmr\n",
        " import pandas as pd\n",
        " import pymrmr\n",
        " selected_columns=pymrmr.mRMR(df, 'MIQ',6)"
      ],
      "metadata": {
        "id": "ElBU8oQpRpVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.feature_selection import SelectKBest\n",
        " from sklearn.feature_selection import chi2\n",
        " X = data.values[:,:-1]\n",
        " Y = data.values[:,-1:]\n",
        " select_model = SelectKBest(score_func=chi2, k=10)\n",
        " fit = select_model.fit(X, Y)\n",
        " chi2_features_values = fit.transform(X)"
      ],
      "metadata": {
        "id": "zXo9nOXpRpYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " kernels = ['Polynomial', 'RBF', 'Sigmoid','Linear']\n",
        " #A function which returns the corresponding SVC model\n",
        " def getClassifier(ktype):\n",
        "  if ktype == 0:\n",
        "      # Polynomial kernal\n",
        "      return SVC(kernel='poly', degree=8, gamma=\"auto\")\n",
        "  elif ktype == 1:\n",
        "      # Radial Basis Function kernal\n",
        "      return SVC(kernel='rbf', gamma=\"auto\")\n",
        "  elif ktype == 2:\n",
        "      # Sigmoid kernal\n",
        "      return SVC(kernel='sigmoid', gamma=\"auto\")\n",
        "  elif ktype == 3:\n",
        "      # Linear kernal\n",
        "      return SVC(kernel='linear', gamma=\"auto\"\n",
        "\n",
        "\n",
        "with open(\"Kernels_Report.txt\", \"w\") as f:\n",
        "  for i in range(4):\n",
        "   # Separate data into test and training sets\n",
        "   X_train, X_test, y_train,y_test= train_test_split(X, y, test_size = 0.05)\n",
        "   # Train a SVCmodelusingdifferent kernal\n",
        "   svclassifier= getClassifier(i)\n",
        "   svclassifier.fit(X_train, y_train)# Make prediction\n",
        "   y_pred = svclassifier.predict(X_test)# Evaluate our model\n",
        "\n",
        "   kernal_name = \"Evaluation:\",kernels[i], \"kernel\"\n",
        "   report = classification_report(y_test,y_pred)\n",
        "   print(kernal_name, file=f)\n",
        "   print(report,file=f)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "r6eXrOddRpa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " {'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
        " 'max_features': ['auto', 'sqrt'],\n",
        " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
        " 'min_samples_split': [2, 5, 10],\n",
        " 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]\n",
        " }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jLzmmrzUZ8s",
        "outputId": "d15a5265-e8c7-4219-96d3-4405475c5b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
              " 'max_features': ['auto', 'sqrt'],\n",
              " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
              " 'min_samples_split': [2, 5, 10],\n",
              " 'min_samples_leaf': [1, 2, 4],\n",
              " 'bootstrap': [True, False]}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.ensemble import RandomForestClassifier\n",
        " clf=RandomForestClassifier(n_estimators= 2000)\n",
        " #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        " clf.fit(X_train,y_train)\n",
        " # prediction on test set\n",
        " y_pred=clf.predict(X_test)\n",
        " n_estimators=[int(x) for x in np.linspace(start= 200,stop= 2000,num= 10)]\n",
        " # Number of features to consider at every split\n",
        " max_features = ['auto', 'sqrt']\n",
        " # Maximum number of levels in tree\n",
        " max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        " max_depth.append(None)\n",
        " # Minimum number of samples required to split a node\n",
        " min_samples_split = [2, 5, 10]\n",
        " # Minimum number of samples required at each leaf node\n",
        " min_samples_leaf = [1, 2, 4]\n",
        " # Method of selecting samples for training each tree\n",
        " bootstrap = [True, False]\n",
        " random_grid = {'n_estimators': n_estimators,\n",
        " 'max_features': max_features,\n",
        " 'max_depth': max_depth,\n",
        " 'min_samples_split': min_samples_split,\n",
        " 'min_samples_leaf': min_samples_leaf,\n",
        " 'bootstrap': bootstrap}\n",
        " # First create the base model to tune\n",
        " rf = RandomForestClassifier()\n",
        " rf_random = RandomizedSearchCV(estimator = rf, param_distributions =\n",
        " random_grid, n_iter= 100, cv= 3, verbose=2, random_state=42, n_jobs =-1)\n",
        " # Fit the random search model\n",
        " rf_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "tTkiajdkRpdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #LSTM and DNN imports\n",
        " 2 import keras\n",
        " 3 import librosa\n",
        " 4 importpandas aspd\n",
        " 5 import tensorflowas tf\n",
        " 6 from sklearn.model_selectionimporttrain_test_split\n",
        " 7 from sklearn.preprocessing importnormalize\n",
        " 8 importwarnings\n",
        " 9 from tensorflow.keras.layers importLSTM,Dense, Dropout, Activation\n",
        " 10 from keras.models importSequential\n",
        " 11 from keras.layers importActivation\n",
        " 12 from sklearn.preprocessing importLabelEncoder\n",
        " 13 from keras.utils.np_utils import to_categorical\n",
        " 14 from sklearn.preprocessing importStandardScaler\n",
        " 15 from keras.callbacks importEarlyStopping\n",
        " 16 fromnumpy importsqrt\n",
        " 17\n",
        " 18\n",
        " 19 #CNNimports\n",
        " 20 from keras.layers importConv1D,GlobalAveragePooling1D, MaxPooling1D\n",
        " 21 from keras_preprocessing.image importImageDataGenerator\n",
        " 22 from keras.layers import BatchNormalization\n",
        " 23 from keras.layers importConv2D,MaxPooling2D\n",
        " 24 from keras importregularizers,optimizers\n",
        " 25 from keras importregularizers"
      ],
      "metadata": {
        "id": "uSxJzPWCRpff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model = Sequential()\n",
        " model.add(Dense(128, activation='relu', kernel_initializer='he_normal',\n",
        " input_shape=(36,)))\n",
        " model.add(Dropout(0.2)) #visible layer\n",
        " model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
        " model.add(Dropout(0.2))\n",
        " model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
        " model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
        " model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
        " model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
        " model.add(Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
        " model.add(Dense(150, activation='softmax'))"
      ],
      "metadata": {
        "id": "Iq9slMvsUwKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #CNN spectogram\n",
        " def images(files):\n",
        " 2\n",
        " 3 # We define the audiofile from therowsof the dataframe when we iterate through\n",
        " 4 # every row ofourdataframe fortrain,valand test\n",
        " 5 audiofile = str(files.path)\n",
        " 6 # Loading the image with nosamplerateto use the original sample rate\n",
        " 7 X, sample_rate = librosa.load(audiofile, sr=None, res_type='kaiser_fast')\n",
        " 8 # Setting the size oftheimage\n",
        " 9 fig = plt.figure(figsize=[1,1])\n",
        " 10 # This is toget rid of the axes and only getthepicture\n",
        " 11 ax = fig.add_subplot(111)\n",
        " 12 ax.axes.get_xaxis().set_visible(False)\n",
        " 13 ax.axes.get_yaxis().set_visible(False)\n",
        " 14 ax.set_frame_on(False)\n",
        " 15 # This is themelspectrogram from the decibels with alinearrelationship\n",
        " 16 # Setting min and maxfrequency toaccount forhumanvoicefrequency\n",
        " 17 S = librosa.feature.melspectrogram(y=X, sr=sample_rate)\n",
        " 18 librosa.display.specshow(librosa.power_to_db(S, ref=np.max),\n",
        " 19 x_axis='time',y_axis='mel', fmin=50, fmax=280)\n",
        " 20\n",
        " 21 name = files.file\n",
        " 22 file = '/content/images/' + str(name) + '.jpg'\n",
        " 23 # Here we finally save theimagefilechoosingtheresolution\n",
        " 24 plt.savefig(file, dpi=500, bbox_inches='tight',pad_inches=0)\n",
        " 25\n",
        " 26 # Here we close theimagebecauseotherwise wegeta\n",
        " 27 #warning saying that the image stays\n",
        " 28 # open and consumes memory\n",
        " 29 plt.close()\n"
      ],
      "metadata": {
        "id": "F_GzkdK9RpjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " train_generator=datagen.flow_from_dataframe( dataframe=train,\n",
        " 2 directory=\"images\",\n",
        " 3 x_col=\"file\", y_col='label',\n",
        " 4 batch_size=40, shuffle=False,\n",
        " 5 class_mode=\"categorical\",\n",
        " 6 target_size=(387,377))"
      ],
      "metadata": {
        "id": "X0cjwa2IU2_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model= Sequential()\n",
        " model.add(Conv2D(filters=128,kernel_size=(5,5),\n",
        " activation='relu',input_shape=(387,377,3)))\n",
        " model.add(BatchNormalization())\n",
        " model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " model.add(Dropout(0.5))\n",
        "\n",
        " model.add(Conv2D(filters=128,kernel_size=(5,5),activation='relu'))\n",
        " model.add(BatchNormalization())\n",
        " model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " model.add(Dropout(0.2))\n",
        "\n",
        " model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
        " model.add(BatchNormalization())\n",
        " model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " model.add(Flatten())\n",
        " model.add(Dense(512, activation='relu'))\n",
        " model.add(Dropout(0.2))\n",
        " model.add(Dense(128, activation='relu'))\n",
        " model.add(Dense(150, activation='softmax'))"
      ],
      "metadata": {
        "id": "meTW9p7CU4Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/OurData.csv')\n",
        " #Y taking the values of the label\n",
        " y = data['id']\n",
        " #X taking the values of features and here takes the MFCCs only\n",
        " x = data.iloc[:,17:].values\n",
        " #Encoding the labels\n",
        " lb = LabelEncoder()\n",
        " y = to_categorical(lb.fit_transform(y))\n",
        " #Split twice to get the validation set\n",
        " X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1,\n",
        " random_state=123, stratify=y)\n",
        " X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        " test_size=0.1, random_state=123)"
      ],
      "metadata": {
        "id": "SLq4a3QRVCZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " input_shape=(20,1)\n",
        " model = keras.Sequential()\n",
        " model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
        " model.add(LSTM(128, return_sequences=True))\n",
        " model.add(Dropout(0.2))\n",
        " model.add(Dense(128, activation='relu'))\n",
        " model.add(Dense(128, activation='relu'))\n",
        " model.add(Dense(64, activation='relu'))\n",
        " model.add(Dense(64, activation='relu'))\n",
        " model.add(Dense(64, activation='relu'))\n",
        " model.add(Dense(150, activation='softmax'))"
      ],
      "metadata": {
        "id": "aOgO8CcqVGcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model.compile(optimizer='adam',loss='CategoricalCrossentropy',metrics=['acc'])\n"
      ],
      "metadata": {
        "id": "y38xrKEXVIGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=100,\n",
        " verbose=1, mode='auto')\n",
        " history = model.fit(X_train, y_train, epochs=50, batch_size=30,\n",
        " validation_data=(X_val, y_val), shuffle=False, callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "GRGDTsReVKnc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}